# Summary #

This is the seminal paper about capsule networks.

# Dynamic Routing Between Capsules #

This is a seminal paper by Sabour, Frosst, and Hinton available on
[arxiv.org][2] ([1710.09829][1]) regarding capsule networks. The authors take
convolutional layers in a neural network and divide groups of neurons into 
capsules. Active capsules will be thought of as nodes in a parse tree. And
active capsules choose a capsule in the next network layer to be its parent. The
output of capsules will be vectors. And it is in this paper that the magnitude
of the vector represents the probability of a feature&rsquo;s existence in the
input, while the orientation of the vector represents the feature&rsquo;s
characteristic.

For all but the first layer of capsules, the input to a capsule
`\(\mathbf{s}_{j}\)` is the weighted sum over all *prediction vectors*
`\(\mathbf{\hat{u}}_{j|i}\)`, which itself is constructed by multiplying the
previous layers output `\(\mathbf{u}_{i}\)` by a weight matrix
`\(\mathbf{W}_{ij}\)`.
`\[\mathbf{\hat{u}}_{j|i} = \mathbf{W}_{ij} \mathbf{u}_{i}\]`
`\[\mathbf{s}_{j} = \sum_{i} c_{ij} \mathbf{\hat{u}}_{j|i}\]`
The final output `\(\mathbf{v}_{j}\)` is normalized so the output is like a 
probability.
`\[\mathbf{v}_{j} = \frac{\|\mathbf{s}_{j}\|^{2}}{1 + \|\mathbf{s}_{j}\|^{2}}
\frac{\mathbf{s}_{j}}{\|\mathbf{s}_{j}\|}\]`
The `\(c_{ij}\)` are the coupling coefficients that are learned through the
dynamic routing process. These coupling coefficients from a capsule `\(i\)` and
all the capsules in the subsequent layer sum to 1 and are determined by a 
*routing softmax* whose initial logits `\(b_{ij}\)` are the log prior
probabilities that capsule `\(i\)` should be routed to capsule `\(j\)`.
`\[c_{ij} = \frac{\exp (b_{ij})}{\sum_{k} \exp (b_{ik})}\]`
The log priors can be learned along with the other weights. The coupling
coefficients can then be refined by measuring the agreement between the current
output `\(\mathbf{v}_{j}\)` of each capsule `\(j\)` in the subsequent layer and
the prediction vector `\(\mathbf{u}_{j|i}\)` made by capsule `\(i\)` in the
current layer. Agreement is measured by the dot product `\(\mathbf{v}_{j}\cdot
\mathbf{u}_{j|i}\)`.

The authors experiment with the MNIST data set and test, among other things, how
resilient capsule networks are towards affine transformations. They tested
capsule networks against traditional convolutional neural networks on a
MNIST-like data set which has been padded to `\(40\times 40\)` and had a MNIST
digit randomly placed on the black background. The results showed that similar
convolutional neural networks performed poorer on this translated MNIST data
set than did the capsule networks.

[1]: https://arxiv.org/abs/1710.09829
[2]: https://arxiv.org/